{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "For this I will be using the \"SemEval 2017 task 4\" My focus is particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None, names=['tweet_id', 'sentiment', 'tweet-text'])\n",
    "    return df\n",
    "\n",
    "# Loading the training set, development set and 3 test sets\n",
    "train_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-training-data.txt')\n",
    "dev_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-dev-data.txt')\n",
    "test1_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test1.txt')\n",
    "test2_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test2.txt')\n",
    "test3_path = os.path.join(os.getcwd(), 'semeval-tweets', 'twitter-test3.txt')\n",
    "\n",
    "train_df = load_pandas_dataframe(train_path)\n",
    "dev_df = load_pandas_dataframe(dev_path)\n",
    "test1_df = load_pandas_dataframe(test1_path)\n",
    "test2_df = load_pandas_dataframe(test2_path)\n",
    "test3_df = load_pandas_dataframe(test3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VatsaL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VatsaL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VatsaL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VatsaL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        pos = 'n'  # default to noun if not found\n",
    "        if tag.startswith('J'):\n",
    "            pos = 'a'  # adjective\n",
    "        elif tag.startswith('V'):\n",
    "            pos = 'v'  # verb\n",
    "        elif tag.startswith('R'):\n",
    "            pos = 'r'  # adverb\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
    "    # Join tokens back into text\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tweet-text'] = train_df['tweet-text'].apply(preprocess_text)\n",
    "dev_df['tweet-text'] = dev_df['tweet-text'].apply(preprocess_text)\n",
    "test1_df['tweet-text'] = test1_df['tweet-text'].apply(preprocess_text)\n",
    "test2_df['tweet-text'] = test2_df['tweet-text'].apply(preprocess_text)\n",
    "test3_df['tweet-text'] = test3_df['tweet-text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['tweet-text']\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "X_dev = dev_df['tweet-text']\n",
    "y_dev = dev_df['sentiment'] \n",
    "\n",
    "X_test1 = test1_df['tweet-text']\n",
    "y_test1 = test1_df['sentiment']\n",
    "\n",
    "X_test2 = test2_df['tweet-text']\n",
    "y_test2 = test2_df['sentiment']\n",
    "\n",
    "X_test3 = test3_df['tweet-text']\n",
    "y_test3 = test3_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training maxent\n",
      "Training maxent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VatsaL\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.49      0.57       378\n",
      "     neutral       0.66      0.70      0.68       919\n",
      "    positive       0.66      0.71      0.68       703\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.67      0.63      0.64      2000\n",
      "weighted avg       0.66      0.66      0.66      2000\n",
      "\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-dev-data.txt (MAXENT): 0.584\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test1.txt (MAXENT): 0.186\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test2.txt (MAXENT): 0.271\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test3.txt (MAXENT): 0.235\n",
      "Training naive_bayes\n",
      "Training naive_bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.22      0.34       378\n",
      "     neutral       0.58      0.75      0.65       919\n",
      "    positive       0.63      0.61      0.62       703\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.62      0.53      0.53      2000\n",
      "weighted avg       0.61      0.60      0.58      2000\n",
      "\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-dev-data.txt (NB): 0.439\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test1.txt (NB): 0.145\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test2.txt (NB): 0.231\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test3.txt (NB): 0.208\n",
      "Training svm\n",
      "Training svm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.53      0.58       378\n",
      "     neutral       0.66      0.68      0.67       919\n",
      "    positive       0.66      0.70      0.68       703\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.65      0.63      0.64      2000\n",
      "weighted avg       0.66      0.66      0.65      2000\n",
      "\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-dev-data.txt (SVM): 0.584\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test1.txt (SVM): 0.183\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test2.txt (SVM): 0.263\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test3.txt (SVM): 0.236\n",
      "TrainingLSTM\n",
      "TrainingLSTM\n",
      "Epoch 1, Loss: 0.7693420648574829\n",
      "Epoch 2, Loss: 0.6302173137664795\n",
      "Epoch 3, Loss: 1.031083345413208\n",
      "Epoch 4, Loss: 1.1272399425506592\n",
      "Epoch 5, Loss: 0.22015736997127533\n",
      "Epoch 6, Loss: 0.19796298444271088\n",
      "Epoch 7, Loss: 0.16194677352905273\n",
      "Epoch 8, Loss: 1.1193954944610596\n",
      "Epoch 9, Loss: 0.5978115797042847\n",
      "Epoch 10, Loss: 0.6389594674110413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.52      0.58       378\n",
      "     neutral       0.62      0.68      0.65       919\n",
      "    positive       0.63      0.62      0.62       703\n",
      "\n",
      "    accuracy                           0.63      2000\n",
      "   macro avg       0.63      0.61      0.62      2000\n",
      "weighted avg       0.63      0.63      0.63      2000\n",
      "\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test1.txt (LSTM): 0.532\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test2.txt (LSTM): 0.542\n",
      "C:\\Users\\VatsaL\\NLP Practical\\semeval-tweets\\twitter-test3.txt (LSTM): 0.514\n"
     ]
    }
   ],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['maxent', 'naive_bayes', 'svm', 'LSTM']:\n",
    "    for features in ['bow', 'glove']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'maxent':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "            if features == 'bow':\n",
    "                pipeline_maxent_clf = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(ngram_range = (1,3))),\n",
    "                    ('clf', LogisticRegression(solver='lbfgs', C=1, max_iter=100, penalty='l2'))\n",
    "                   ])\n",
    "            elif features == 'glove':\n",
    "                pass\n",
    "                pipeline_maxent_clf.fit(X_train,y_train)\n",
    "                y_pred_maxent = pipeline_maxent_clf.predict(X_dev)\n",
    "                print(classification_report(y_dev, y_pred_maxent))\n",
    "                dev_tweet_ids = dev_df['tweet_id'].tolist()\n",
    "                predictions_dev_maxent = {tweet_id: pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_maxent)}\n",
    "                predictions_dev_maxent = {str(tweet_id): pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_maxent)}\n",
    "                evaluate(predictions_dev_maxent, dev_path, 'MAXENT')\n",
    "\n",
    "                test1_tweet_ids = test1_df['tweet_id'].tolist()\n",
    "                predictions_test1_maxent = {tweet_id: pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_maxent)}\n",
    "                predictions_test1_maxent = {str(tweet_id): pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_maxent)}\n",
    "\n",
    "                test2_tweet_ids = test2_df['tweet_id'].tolist()\n",
    "                predictions_test2_maxent = {tweet_id: pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_maxent)}\n",
    "                predictions_test2_maxent = {str(tweet_id): pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_maxent)}\n",
    "\n",
    "                test3_tweet_ids = test3_df['tweet_id'].tolist()\n",
    "                predictions_test3_maxent = {tweet_id: pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_maxent)}\n",
    "                predictions_test3_maxent = {str(tweet_id): pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_maxent)}\n",
    "\n",
    "                evaluate(predictions_test1_maxent, test1_path, 'MAXENT')\n",
    "                evaluate(predictions_test2_maxent, test2_path, 'MAXENT')\n",
    "                evaluate(predictions_test3_maxent, test3_path, 'MAXENT')\n",
    "                \n",
    "        elif classifier == 'naive_bayes':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "            if features == 'bow':\n",
    "                pipeline_NB_clf = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(ngram_range = (1,3))),\n",
    "                    ('clf', MultinomialNB(alpha =0.1))\n",
    "                   ])\n",
    "            elif features == 'glove':\n",
    "                pass\n",
    "                pipeline_NB_clf.fit(X_train,y_train)\n",
    "                y_pred_NB = pipeline_NB_clf.predict(X_dev)\n",
    "                print(classification_report(y_dev, y_pred_NB))\n",
    "                dev_tweet_ids = dev_df['tweet_id'].tolist()\n",
    "                predictions_dev_NB = {tweet_id: pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_NB)}\n",
    "                predictions_dev_NB = {str(tweet_id): pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_NB)}\n",
    "                evaluate(predictions_dev_NB, dev_path, 'NB')\n",
    "                test1_tweet_ids = test1_df['tweet_id'].tolist()\n",
    "                predictions_test1_NB = {tweet_id: pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_NB)}\n",
    "                predictions_test1_NB = {str(tweet_id): pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_NB)}\n",
    "\n",
    "                test2_tweet_ids = test2_df['tweet_id'].tolist()\n",
    "                predictions_test2_NB = {tweet_id: pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_NB)}\n",
    "                predictions_test2_NB = {str(tweet_id): pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_NB)}\n",
    "\n",
    "                test3_tweet_ids = test3_df['tweet_id'].tolist()\n",
    "                predictions_test3_NB = {tweet_id: pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_NB)}\n",
    "                predictions_test3_NB = {str(tweet_id): pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_NB)}\n",
    "\n",
    "                evaluate(predictions_test1_NB, test1_path, 'NB')\n",
    "                evaluate(predictions_test2_NB, test2_path, 'NB')\n",
    "                evaluate(predictions_test3_NB, test3_path, 'NB')\n",
    "        elif classifier == 'svm':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "            if features == 'bow':\n",
    "                pipeline_SVM_clf = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(ngram_range = (1,3))),\n",
    "                    ('clf', SVC(kernel = 'linear', C = 1 ))\n",
    "                   ])\n",
    "            elif features == 'glove':\n",
    "                pass\n",
    "                pipeline_SVM_clf.fit(X_train,y_train)\n",
    "                y_pred_SVM = pipeline_SVM_clf.predict(X_dev)\n",
    "                print(classification_report(y_dev, y_pred_SVM))\n",
    "\n",
    "                dev_tweet_ids = dev_df['tweet_id'].tolist()\n",
    "                predictions_dev_SVM = {tweet_id: pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_SVM)}\n",
    "                predictions_dev_SVM = {str(tweet_id): pred for tweet_id, pred in zip(dev_tweet_ids, y_pred_SVM)}\n",
    "                evaluate(predictions_dev_SVM, dev_path, 'SVM')\n",
    "\n",
    "                test1_tweet_ids = test1_df['tweet_id'].tolist()\n",
    "                predictions_test1_SVM = {tweet_id: pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_SVM)}\n",
    "                predictions_test1_SVM = {str(tweet_id): pred for tweet_id, pred in zip(test1_tweet_ids, y_pred_SVM)}\n",
    "\n",
    "                test2_tweet_ids = test2_df['tweet_id'].tolist()\n",
    "                predictions_test2_SVM = {tweet_id: pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_SVM)}\n",
    "                predictions_test2_SVM = {str(tweet_id): pred for tweet_id, pred in zip(test2_tweet_ids, y_pred_SVM)}\n",
    "\n",
    "                test3_tweet_ids = test3_df['tweet_id'].tolist()\n",
    "                predictions_test3_SVM = {tweet_id: pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_SVM)}\n",
    "                predictions_test3_SVM = {str(tweet_id): pred for tweet_id, pred in zip(test3_tweet_ids, y_pred_SVM)}\n",
    "\n",
    "                evaluate(predictions_test1_SVM, test1_path, 'SVM')\n",
    "                evaluate(predictions_test2_SVM, test2_path, 'SVM')\n",
    "                evaluate(predictions_test3_SVM, test3_path, 'SVM')\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the classifier 4 here\n",
    "            print('Training' + classifier)\n",
    "            if features == 'bow':\n",
    "                pass\n",
    "            elif features == 'glove':\n",
    "                def load_glove_embeddings(path):\n",
    "                    embeddings_dict = {}\n",
    "                    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "                        for line in f:\n",
    "                            values = line.split()\n",
    "                            word = values[0]\n",
    "                            vector = np.asarray(values[1:], \"float32\")\n",
    "                            embeddings_dict[word] = vector\n",
    "                    return embeddings_dict\n",
    "\n",
    "                glove_path = os.path.join(os.getcwd(), \"glove.6B\", \"glove.6B.100d.txt\")\n",
    "                glove_embeddings = load_glove_embeddings(glove_path)\n",
    "                \n",
    "                \n",
    "                X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
    "                X_dev_tokens = X_dev.apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "                word_index = {}\n",
    "                current_index = 1\n",
    "                for tokens in X_train_tokens:\n",
    "                    for token in tokens:\n",
    "                        if token not in word_index:\n",
    "                            word_index[token] = current_index\n",
    "                            current_index += 1\n",
    "\n",
    "                X_train_seq = [[word_index[token] for token in tokens if token in word_index] for tokens in X_train_tokens]\n",
    "                X_dev_seq = [[word_index[token] for token in tokens if token in word_index] for tokens in X_dev_tokens]\n",
    "\n",
    "                max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_dev_seq))\n",
    "                \n",
    "                def pading_sequences(sequences, maxlen):\n",
    "                    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)\n",
    "                    for i, seq in enumerate(sequences):\n",
    "                        len_seq = len(seq)\n",
    "                        if len_seq > maxlen:\n",
    "                            padded_sequences[i, :] = seq[:maxlen]\n",
    "                        elif len_seq > 0:\n",
    "                            padded_sequences[i, -len_seq:] = seq\n",
    "                    return padded_sequences\n",
    "                \n",
    "                X_train = pading_sequences(X_train_seq, maxlen=max_length)\n",
    "                X_dev = pading_sequences(X_dev_seq, maxlen=max_length)\n",
    "                \n",
    "                vocab_size = len(word_index) + 1\n",
    "                embedding_dim = 100\n",
    "\n",
    "                embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "                for word, i in word_index.items():\n",
    "                    embedding_vector = glove_embeddings.get(word)\n",
    "                    if embedding_vector is not None:\n",
    "                        embedding_matrix[i] = embedding_vector\n",
    "                    \n",
    "                \n",
    "\n",
    "                class LSTMClassifier(nn.Module):\n",
    "                  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
    "                    super(LSTMClassifier, self).__init__()\n",
    "\n",
    "                    # Hyperparameters\n",
    "                    self.vocab_size = vocab_size\n",
    "                    self.embedding_dim = embedding_dim\n",
    "                    self.hidden_dim = hidden_dim\n",
    "                    self.output_dim = output_dim\n",
    "\n",
    "                    # Embedding layer\n",
    "                    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "                    self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "                    self.embedding.weight.requires_grad = False  # Freeze pre-trained embeddings\n",
    "\n",
    "                    # LSTM layer (batch_first=True)\n",
    "                    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "                    # Dense layer for output\n",
    "                    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "                  def forward(self, text):\n",
    "                    # Get embeddings from input text\n",
    "                    embedded = self.embedding(text)\n",
    "\n",
    "                    # Pass embeddings through LSTM\n",
    "                    _, (hidden, _) = self.lstm(embedded)\n",
    "\n",
    "                    # Get last hidden state\n",
    "                    hidden = hidden[-1]\n",
    "\n",
    "                    # Pass last hidden state through dense layer\n",
    "                    output = self.fc(hidden)\n",
    "\n",
    "                    return output\n",
    "\n",
    "\n",
    "                lstm_model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim=128, output_dim=3, embedding_matrix=embedding_matrix)\n",
    "                \n",
    "                label_encoder = LabelEncoder()\n",
    "                y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "                y_dev_encoded = label_encoder.transform(y_dev)\n",
    "                \n",
    "                # Convert data to PyTorch tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype = torch.long)\n",
    "                X_dev_tensor = torch.tensor(X_dev, dtype = torch.long)\n",
    "                y_train_tensor = torch.tensor(y_train_encoded, dtype = torch.long)\n",
    "                y_dev_tensor = torch.tensor(y_dev_encoded, dtype = torch.long)\n",
    "                \n",
    "                train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "                train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "                \n",
    "                num_epochs = 10\n",
    "                lstm_model.train()\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    for inputs, labels in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        output = lstm_model(inputs)\n",
    "                        loss = criterion(output, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "                \n",
    "                lstm_model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predictions_dev_lstm = lstm_model(X_dev_tensor)\n",
    "                    _, predicted_labels = torch.max(predictions_dev_lstm, 1)\n",
    "                print(classification_report(y_dev_tensor.numpy(), predicted_labels.numpy(), target_names=label_encoder.classes_))\n",
    "                \n",
    "                X_test1_tokens = X_test1.apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "                X_test1_seq = [[word_index[token] for token in tokens if token in word_index] for tokens in X_test1_tokens]\n",
    "                X_test1_pad = pading_sequences(X_test1_seq, maxlen=max_length)\n",
    "\n",
    "                y_test1_encoded = label_encoder.transform(y_test1)\n",
    "\n",
    "                X_test1_tensor = torch.tensor(X_test1_pad, dtype=torch.long)\n",
    "                y_test1_tensor = torch.tensor(y_test1_encoded, dtype=torch.long)\n",
    "\n",
    "                lstm_model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predictions_test1 = lstm_model(X_test1_tensor)\n",
    "                    _, predicted_labels_test1 = torch.max(predictions_test1, 1)\n",
    "                predicted_labels_test1 = [label_encoder.inverse_transform([idx.item()])[0] for idx in predicted_labels_test1]\n",
    "                predictions_test1_lstm = {str(tweet_id): pred for tweet_id, pred in zip(test1_tweet_ids, predicted_labels_test1)}\n",
    "                evaluate(predictions_test1_lstm, test1_path, 'LSTM')\n",
    "                \n",
    "                X_test2_tokens = X_test2.apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "                X_test2_seq = [[word_index[token] for token in tokens if token in word_index] for tokens in X_test2_tokens]\n",
    "                X_test2_pad = pading_sequences(X_test2_seq, maxlen=max_length)\n",
    "\n",
    "                y_test2_encoded = label_encoder.transform(y_test2)\n",
    "\n",
    "                X_test2_tensor = torch.tensor(X_test2_pad, dtype=torch.long)\n",
    "                y_test2_tensor = torch.tensor(y_test2_encoded, dtype=torch.long)\n",
    "\n",
    "                lstm_model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predictions_test2 = lstm_model(X_test2_tensor)\n",
    "                    _, predicted_labels_test2 = torch.max(predictions_test2, 1)\n",
    "                predicted_labels_test2 = [label_encoder.inverse_transform([idx.item()])[0] for idx in predicted_labels_test2]\n",
    "                predictions_test2_lstm = {str(tweet_id): pred for tweet_id, pred in zip(test2_tweet_ids, predicted_labels_test2)}\n",
    "                evaluate(predictions_test2_lstm, test2_path, 'LSTM')\n",
    "                \n",
    "                X_test3_tokens = X_test3.apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "                X_test3_seq = [[word_index[token] for token in tokens if token in word_index] for tokens in X_test3_tokens]\n",
    "                X_test3_pad = pading_sequences(X_test3_seq, maxlen=max_length)\n",
    "\n",
    "                y_test3_encoded = label_encoder.transform(y_test3)\n",
    "\n",
    "                X_test3_tensor = torch.tensor(X_test3_pad, dtype=torch.long)\n",
    "                y_test3_tensor = torch.tensor(y_test3_encoded, dtype=torch.long)\n",
    "\n",
    "                lstm_model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    predictions_test3 = lstm_model(X_test3_tensor)\n",
    "                    _, predicted_labels_test3 = torch.max(predictions_test3, 1)\n",
    "                predicted_labels_test3 = [label_encoder.inverse_transform([idx.item()])[0] for idx in predicted_labels_test3]\n",
    "                predictions_test3_lstm = {str(tweet_id): pred for tweet_id, pred in zip(test3_tweet_ids, predicted_labels_test3)}\n",
    "                evaluate(predictions_test3_lstm, test3_path, 'LSTM')\n",
    "\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        #for testset in testsets:\n",
    "            #id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            #testset_name = testset\n",
    "            #testset_path = join('semeval-tweets', testset_name)\n",
    "            #evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
